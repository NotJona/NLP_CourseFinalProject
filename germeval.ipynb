{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setup ",
   "id": "63182982ad0e4f0c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-23T08:08:45.032363Z",
     "start_time": "2025-08-23T08:08:39.359283Z"
    }
   },
   "source": "!pip install transformers[torch] datasets scikit-learn pandas numpy",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (4.55.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from transformers[torch]) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from transformers[torch]) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from transformers[torch]) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from transformers[torch]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from transformers[torch]) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from transformers[torch]) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from transformers[torch]) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from transformers[torch]) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.1 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from transformers[torch]) (2.8.0)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from transformers[torch]) (1.10.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (7.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (4.14.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from torch>=2.1->transformers[torch]) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from torch>=2.1->transformers[torch]) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from torch>=2.1->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from torch>=2.1->transformers[torch]) (80.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from requests->transformers[torch]) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from requests->transformers[torch]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from requests->transformers[torch]) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.1->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jona\\pycharmprojects\\germevalproject\\.venv\\lib\\site-packages (from jinja2->torch>=2.1->transformers[torch]) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:08:57.204008Z",
     "start_time": "2025-08-23T08:08:45.438520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import json\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np"
   ],
   "id": "535a82afb6a7a671",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jona\\PycharmProjects\\GermEvalProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Let us load our data",
   "id": "58ed7243d913dcf9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:08:57.213424Z",
     "start_time": "2025-08-23T08:08:57.209548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Load a JSONL file and return a list of JSON objects.\n",
    "    :param file_path: str, path to the JSONL file\n",
    "    :return: list of dicts, each representing a JSON object\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    return data"
   ],
   "id": "3a7aa88754d97ea3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:08:57.226773Z",
     "start_time": "2025-08-23T08:08:57.222598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_file_path = 'data_germeval/train.jsonl'\n",
    "dev_file_path = 'data_germeval/development.jsonl'\n",
    "test_file_path = 'data_germeval/test.jsonl'"
   ],
   "id": "fdfa2212bc8ffdd2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:08:57.266436Z",
     "start_time": "2025-08-23T08:08:57.233566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data = load_jsonl(train_file_path)\n",
    "dev_data = load_jsonl(dev_file_path)\n",
    "test_data = load_jsonl(test_file_path)"
   ],
   "id": "ed58534c3d916130",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Let us define the functions that compute the different labels for the germeval task",
   "id": "cde1143fdad68f12"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Version \"bin_maj\"",
   "id": "4d251101693bdfa5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:08:57.789191Z",
     "start_time": "2025-08-23T08:08:57.784685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def assign_bin_maj(item, is_test=False):\n",
    "    \"\"\"\n",
    "    takes a tweet and its annotations (if available) and computes 1 if a majority of annotators assigned a label other than 0-Kein, predicts 0 if a majority assigned 0-Kein. If there was no majority, either label is considered correct for evaluation.\n",
    "    :param item: dictionary of the form {'id': , 'text': , 'annotators': }\n",
    "    :param is_test: if False annotations are available. If True not\n",
    "    :return: dictionary of the form {'id': , 'text': , 'label': }\n",
    "    \"\"\"\n",
    "    text = item['text']\n",
    "    text = text.replace('\\n', ' ')\n",
    "    if not is_test:\n",
    "        labels = [ann['label'] for ann in item['annotations']]\n",
    "        label_counts = Counter(labels)\n",
    "        majority_label, majority_count = label_counts.most_common(1)[0]\n",
    "        bin_maj_label = 1 if majority_label != '0-Kein' else 0\n",
    "    else:\n",
    "        bin_maj_label = None\n",
    "    return {'id': item['id'], 'text': text, 'label': bin_maj_label}\n",
    "    "
   ],
   "id": "bfc6ab773fa0752d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Version \"bin_one\"",
   "id": "1e2d29637f5a2c72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:08:57.828877Z",
     "start_time": "2025-08-23T08:08:57.823355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def assign_bin_one(item, is_test=False):\n",
    "    \"\"\"\n",
    "    takes a tweet and its annotations (if available) and computes 1 if at least one annotator assigned a label other than 0-Kein, 0 otherwise.\n",
    "    :param item: dictionary of the form {'id': , 'text': , 'annotators': }\n",
    "    :param is_test: if False annotations are available. If True not\n",
    "    :return: dictionary of the form {'id': , 'text': , 'label': }\n",
    "    \"\"\"\n",
    "    text = item['text']\n",
    "    text = text.replace('\\n', ' ')\n",
    "    if not is_test:\n",
    "        bin_one_label = 1 if any(ann['label'] != '0-Kein' for ann in item['annotations']) else 0\n",
    "    else:\n",
    "        bin_one_label = None\n",
    "    return {'id': item['id'], 'text': text, 'label': bin_one_label}"
   ],
   "id": "82fbad55bd5c0691",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Version \"bin_all\"",
   "id": "27b5981245ce690c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:08:57.845957Z",
     "start_time": "2025-08-23T08:08:57.840909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def assign_bin_all(item, is_test=False):\n",
    "    \"\"\"\n",
    "    takes a tweet and its annotations (if available) and computes 1 if all annotators assigned labels other than 0-Kein, 0 otherwise.\n",
    "    :param item: dictionary of the form {'id': , 'text': , 'annotators': }\n",
    "    :param is_test: if False annotations are available. If True not\n",
    "    :return: dictionary of the form {'id': , 'text': , 'label': }\n",
    "    \"\"\"\n",
    "    text = item['text']\n",
    "    text = text.replace('\\n', ' ')\n",
    "    if not is_test:\n",
    "        bin_all_label = 1 if all(ann['label'] != '0-Kein' for ann in item['annotations']) else 0\n",
    "    else:\n",
    "        bin_all_label = None\n",
    "    return {'id': item['id'], 'text': text, 'label': bin_all_label}"
   ],
   "id": "1bded10a91c6926c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Version \"multi_maj\"",
   "id": "136a784630eba323"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:08:57.858488Z",
     "start_time": "2025-08-23T08:08:57.852983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def assign_multi_maj(item, is_test=False):\n",
    "    \"\"\"\n",
    "    takes a tweet and its annotations (if available) and predicts the majority label if there is one, if there is no majority label, any of the labels assigned is counted as a correct prediction for evaluation.\n",
    "    :param item: dictionary of the form {'id': , 'text': , 'annotators': }\n",
    "    :param is_test: if False annotations are available. If True not\n",
    "    :return: dictionary of the form {'id': , 'text': , 'label': }\n",
    "    \"\"\"\n",
    "    text = item['text']\n",
    "    text = text.replace('\\n', ' ')\n",
    "    if not is_test:\n",
    "        labels = [ann['label'] for ann in item['annotations']]\n",
    "        label_counts = Counter(labels)\n",
    "        majority_label, majority_count = label_counts.most_common(1)[0]\n",
    "        multi_maj_label = majority_label if majority_count > len(labels) / 2 else labels[0]\n",
    "        multi_maj_label = int(multi_maj_label.split('-')[0])\n",
    "    else:\n",
    "        multi_maj_label = None\n",
    "    return {'id': item['id'], 'text': text, 'label': multi_maj_label}"
   ],
   "id": "235981f629474fab",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Version \"disagree bin\"",
   "id": "e22062f52f37b9f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:08:57.870536Z",
     "start_time": "2025-08-23T08:08:57.865745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def assign_disagree_bin(item, is_test=False):\n",
    "    \"\"\"\n",
    "    takes a tweet and its annotations (if available) and predicts 1 if there is a disagreement between annotators on 0-Kein versus all other labels and 0 otherwise.\n",
    "    :param item: dictionary of the form {'id': , 'text': , 'annotators': }\n",
    "    :param is_test: if False annotations are available. If True not\n",
    "    :return: dictionary of the form {'id': , 'text': , 'label': }\n",
    "    \"\"\"\n",
    "    text = item['text']\n",
    "    text = text.replace('\\n', ' ')\n",
    "    if not is_test:\n",
    "        labels = [ann['label'] for ann in item['annotations']]\n",
    "        unique_labels = set(labels)\n",
    "        disagree_bin_label = 1 if '0-Kein' in unique_labels and len(unique_labels) > 1 else 0\n",
    "    else:\n",
    "        disagree_bin_label = None\n",
    "    return {'id': item['id'], 'text': text, 'label': disagree_bin_label}"
   ],
   "id": "df98e947fb06d5b0",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Let us not define the function, that transforms our data into the suitable objects, i.e., huggingface datasets",
   "id": "65b5b5be01741c1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:08:57.883352Z",
     "start_time": "2025-08-23T08:08:57.878307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def transform(func, data, is_test=False):\n",
    "    \"\"\"\n",
    "    Computes a particular label for a whole set of data\n",
    "    :param func: one of the five functions defined above\n",
    "    :param data: list of dictionaries\n",
    "    :param is_test: if False annotations are available. If True not\n",
    "    :return: huggingface dataset  \n",
    "    \"\"\"\n",
    "    transformed_data = []\n",
    "    for item in data:\n",
    "        transformed_data.append(func(item, is_test))\n",
    "    return Dataset.from_list(transformed_data)"
   ],
   "id": "d9dd72223c670fbf",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Now we have to load the tokenizer and then we can train our models",
   "id": "3ee55f775f12679d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:08:58.724371Z",
     "start_time": "2025-08-23T08:08:57.891469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"google-bert/bert-base-german-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ],
   "id": "2e6cf8ad29759a49",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:08:58.741016Z",
     "start_time": "2025-08-23T08:08:58.737080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_seqs(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,   # or padding=True\n",
    "        max_length=512\n",
    "    )"
   ],
   "id": "2afdd00f0c772e87",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "do we want to train?",
   "id": "1eb973e5e1a0390d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:08:58.753271Z",
     "start_time": "2025-08-23T08:08:58.749384Z"
    }
   },
   "cell_type": "code",
   "source": "training = False",
   "id": "e2a85d5c88c6134",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preparing for training",
   "id": "759d13b6e75c3956"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's define our metrics for evaluation:",
   "id": "40ba68746d9ac6c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:08:58.765046Z",
     "start_time": "2025-08-23T08:08:58.761978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds = eval_preds.predictions.argmax(-1)\n",
    "    lbls = eval_preds.label_ids\n",
    "    f1 = f1_score(lbls, preds, average='weighted')\n",
    "    return {'f1': f1}"
   ],
   "id": "aba704c828212599",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Let's set up our bin_maj data and model:",
   "id": "bf80ded14d8a8be6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:26:10.515797Z",
     "start_time": "2025-08-23T08:26:06.697297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train_bin_maj = transform(assign_bin_maj, train_data).map(tokenize_seqs, batched=True)\n",
    "tokenized_dev_bin_maj = transform(assign_bin_maj, dev_data).map(tokenize_seqs, batched=True)\n",
    "tokenized_test_bin_maj = transform(assign_bin_maj, test_data, is_test= True).map(tokenize_seqs, batched=True)"
   ],
   "id": "fd3d1aa36cf8fd68",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3588/3588 [00:02<00:00, 1330.33 examples/s]\n",
      "Map: 100%|██████████| 449/449 [00:00<00:00, 2005.72 examples/s]\n",
      "Map: 100%|██████████| 449/449 [00:00<00:00, 1952.15 examples/s]\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:09:03.969868Z",
     "start_time": "2025-08-23T08:09:03.056051Z"
    }
   },
   "cell_type": "code",
   "source": "model_bin_maj = BertForSequenceClassification.from_pretrained(\"google-bert/bert-base-german-cased\", num_labels = 2)",
   "id": "29b43fd85533d3d9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:09:04.025296Z",
     "start_time": "2025-08-23T08:09:04.007121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args_bin_maj= TrainingArguments(\n",
    "    output_dir='./logs/run_final_bin_maj/',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=5e-3,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps= 30,\n",
    "    save_strategy='epoch',\n",
    "    eval_strategy='epoch',\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",  \n",
    "    dataloader_pin_memory=False\n",
    ")"
   ],
   "id": "db8c29a93ce5b107",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:09:04.070356Z",
     "start_time": "2025-08-23T08:09:04.041063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer_bin_maj = Trainer(\n",
    "    model=model_bin_maj,\n",
    "    args=training_args_bin_maj,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=tokenized_train_bin_maj,\n",
    "    eval_dataset=tokenized_dev_bin_maj,\n",
    "    processing_class = tokenizer\n",
    ")"
   ],
   "id": "985fa06c9dcd9661",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:09:04.088522Z",
     "start_time": "2025-08-23T08:09:04.083017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if training:\n",
    "    trainer_bin_maj.train()\n",
    "    model_path = \"models/bin_maj_model\"\n",
    "    trainer_bin_maj.save_model(model_path)"
   ],
   "id": "debc063a9c5bf6f4",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Let's set up our bin_one data and model:",
   "id": "4795eb1918748c04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:26:15.649957Z",
     "start_time": "2025-08-23T08:26:11.528986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train_bin_one = transform(assign_bin_one, train_data).map(tokenize_seqs, batched=True)\n",
    "tokenized_dev_bin_one = transform(assign_bin_one, dev_data).map(tokenize_seqs, batched=True)\n",
    "tokenized_test_bin_one = transform(assign_bin_one, test_data, is_test=True).map(tokenize_seqs, batched=True)"
   ],
   "id": "f37034e8a77193a5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3588/3588 [00:02<00:00, 1216.63 examples/s]\n",
      "Map: 100%|██████████| 449/449 [00:00<00:00, 1796.64 examples/s]\n",
      "Map: 100%|██████████| 449/449 [00:00<00:00, 1282.16 examples/s]\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:09:08.150861Z",
     "start_time": "2025-08-23T08:09:07.677501Z"
    }
   },
   "cell_type": "code",
   "source": "model_bin_one = BertForSequenceClassification.from_pretrained(\"google-bert/bert-base-german-cased\", num_labels = 2)",
   "id": "ed6ae1b2dc8eee36",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:09:08.178482Z",
     "start_time": "2025-08-23T08:09:08.169326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args_bin_one= TrainingArguments(\n",
    "    output_dir='./logs/run_final_bin_one/',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=5e-3,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps= 30,\n",
    "    save_strategy='epoch',\n",
    "    eval_strategy='epoch',\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",  \n",
    "    dataloader_pin_memory=False\n",
    ")"
   ],
   "id": "6de3b2ab593ef908",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:09:08.209588Z",
     "start_time": "2025-08-23T08:09:08.196957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer_bin_one = Trainer(\n",
    "    model=model_bin_one,\n",
    "    args=training_args_bin_one,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=tokenized_train_bin_one,\n",
    "    eval_dataset=tokenized_dev_bin_one,\n",
    "    processing_class = tokenizer\n",
    ")"
   ],
   "id": "fb203c4c4b999cf9",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:09:08.223857Z",
     "start_time": "2025-08-23T08:09:08.219561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if training:\n",
    "    trainer_bin_one.train()\n",
    "    model_path = \"models/bin_one_model\"\n",
    "    trainer_bin_one.save_model(model_path)"
   ],
   "id": "d6354cb315fa3b54",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Let's set up our bin_all data and model:",
   "id": "fdb2755c95f8ec7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:09:12.123253Z",
     "start_time": "2025-08-23T08:09:08.242028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train_bin_all = transform(assign_bin_all, train_data).map(tokenize_seqs, batched=True)\n",
    "tokenized_dev_bin_all = transform(assign_bin_all, dev_data).map(tokenize_seqs, batched=True)\n",
    "tokenized_test_bin_all = transform(assign_bin_all, test_data, is_test=True).map(tokenize_seqs, batched=True)"
   ],
   "id": "e7b8b40c6ebc042d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3588/3588 [00:02<00:00, 1387.87 examples/s]\n",
      "Map: 100%|██████████| 449/449 [00:00<00:00, 1191.41 examples/s]\n",
      "Map: 100%|██████████| 449/449 [00:00<00:00, 1678.94 examples/s]\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:09:12.674474Z",
     "start_time": "2025-08-23T08:09:12.142611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_bin_all = BertForSequenceClassification.from_pretrained(\"google-bert/bert-base-german-cased\", num_labels=2)\n",
    "training_args_bin_all = TrainingArguments(\n",
    "    output_dir='./logs/run_final_bin_all/',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=5e-3,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=30,\n",
    "    save_strategy='epoch',\n",
    "    eval_strategy='epoch',\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    dataloader_pin_memory=False\n",
    ")"
   ],
   "id": "fccfc02b4735bb03",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:09:12.775035Z",
     "start_time": "2025-08-23T08:09:12.727630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer_bin_all = Trainer(\n",
    "    model=model_bin_all,\n",
    "    args=training_args_bin_all,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=tokenized_train_bin_all,\n",
    "    eval_dataset=tokenized_dev_bin_all,\n",
    "    processing_class=tokenizer\n",
    ")"
   ],
   "id": "99ffc1fec4efed5",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:09:12.796718Z",
     "start_time": "2025-08-23T08:09:12.792727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if training:\n",
    "    trainer_bin_all.train()\n",
    "    model_path = \"models/bin_all_model\"\n",
    "    trainer_bin_all.save_model(model_path)"
   ],
   "id": "97ee4475e7c57031",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Let's set up our multi_maj data and model:",
   "id": "92de757d5b28c0ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:09:15.899682Z",
     "start_time": "2025-08-23T08:09:12.817111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train_multi_maj = transform(assign_multi_maj, train_data).map(tokenize_seqs, batched=True)\n",
    "tokenized_dev_multi_maj = transform(assign_multi_maj, dev_data).map(tokenize_seqs, batched=True)\n",
    "tokenized_test_multi_maj = transform(assign_multi_maj, test_data, is_test=True).map(tokenize_seqs, batched=True)"
   ],
   "id": "7f6c9bcc966d8a3a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3588/3588 [00:01<00:00, 1947.05 examples/s]\n",
      "Map: 100%|██████████| 449/449 [00:00<00:00, 1389.22 examples/s]\n",
      "Map: 100%|██████████| 449/449 [00:00<00:00, 1279.60 examples/s]\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:09:16.531632Z",
     "start_time": "2025-08-23T08:09:15.986682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_multi_maj = BertForSequenceClassification.from_pretrained(\"google-bert/bert-base-german-cased\", num_labels=5)\n",
    "training_args_multi_maj = TrainingArguments(\n",
    "    output_dir='./logs/run_final_multi_maj/',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=5e-3,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=30,\n",
    "    save_strategy='epoch',\n",
    "    eval_strategy='epoch',\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    dataloader_pin_memory=False\n",
    ")"
   ],
   "id": "83fdc6d2b2218faf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:09:16.678844Z",
     "start_time": "2025-08-23T08:09:16.635720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer_multi_maj = Trainer(\n",
    "    model=model_multi_maj,\n",
    "    args=training_args_multi_maj,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=tokenized_train_multi_maj,\n",
    "    eval_dataset=tokenized_dev_multi_maj,\n",
    "    processing_class=tokenizer\n",
    ")"
   ],
   "id": "eae1e40e0ca9d933",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:09:16.753904Z",
     "start_time": "2025-08-23T08:09:16.746768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if training:\n",
    "    trainer_multi_maj.train()\n",
    "    model_path = \"models/multi_maj_model\"\n",
    "    trainer_multi_maj.save_model(model_path)"
   ],
   "id": "d39fc4fe67edc5ee",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Let's set up our disagree_bin data and model:",
   "id": "902bfc62ff88937d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:09:20.239347Z",
     "start_time": "2025-08-23T08:09:16.802538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train_disagree_bin = transform(assign_disagree_bin, train_data).map(tokenize_seqs, batched=True)\n",
    "tokenized_dev_disagree_bin = transform(assign_disagree_bin, dev_data).map(tokenize_seqs, batched=True)\n",
    "tokenized_test_disagree_bin = transform(assign_disagree_bin, test_data, is_test=True).map(tokenize_seqs, batched=True)"
   ],
   "id": "3f8f3ba6f65ca124",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3588/3588 [00:02<00:00, 1757.55 examples/s]\n",
      "Map: 100%|██████████| 449/449 [00:00<00:00, 1156.88 examples/s]\n",
      "Map: 100%|██████████| 449/449 [00:00<00:00, 1840.66 examples/s]\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:09:20.686669Z",
     "start_time": "2025-08-23T08:09:20.257640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_disagree_bin = BertForSequenceClassification.from_pretrained(\"google-bert/bert-base-german-cased\", num_labels=2)\n",
    "training_args_disagree_bin = TrainingArguments(\n",
    "    output_dir='./logs/run_final_disagree_bin/',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=5e-3,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=30,\n",
    "    save_strategy='epoch',\n",
    "    eval_strategy='epoch',\n",
    "    save_total_limit=1,\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    dataloader_pin_memory=False\n",
    ")"
   ],
   "id": "3183fcad45b4662",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:09:20.811552Z",
     "start_time": "2025-08-23T08:09:20.789493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer_disagree_bin = Trainer(\n",
    "    model=model_disagree_bin,\n",
    "    args=training_args_disagree_bin,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=tokenized_train_disagree_bin,\n",
    "    eval_dataset=tokenized_dev_disagree_bin,\n",
    "    processing_class=tokenizer\n",
    ")"
   ],
   "id": "283c6dfc712f5d1",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:09:20.832490Z",
     "start_time": "2025-08-23T08:09:20.825788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if training:\n",
    "    trainer_disagree_bin.train()\n",
    "    model_path = \"models/disagree_bin_model\"\n",
    "    trainer_disagree_bin.save_model(model_path)"
   ],
   "id": "214273b878dca8d8",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Let us make some predictions!",
   "id": "f9f40e267b81e286"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# First we have to load the different models",
   "id": "1c55a969ff81507e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:09:21.299428Z",
     "start_time": "2025-08-23T08:09:20.850042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = \"models/bin_maj_model\"\n",
    "model_bin_maj = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# rebuild the Trainer (with same args/tokenizer you used before)\n",
    "trainer_bin_maj = Trainer(\n",
    "    model=model_bin_maj,\n",
    "    args=training_args_bin_maj,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=tokenized_train_bin_maj,\n",
    "    eval_dataset=tokenized_dev_bin_maj,\n",
    "    processing_class = tokenizer\n",
    ")\n",
    "\n",
    "model_path = \"models/bin_all_model\"\n",
    "model_bin_all = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# rebuild the Trainer (with same args/tokenizer you used before)\n",
    "trainer_bin_all = Trainer(\n",
    "    model=model_bin_all,\n",
    "    args=training_args_bin_all,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=tokenized_train_bin_all,\n",
    "    eval_dataset=tokenized_dev_bin_all,\n",
    "    processing_class = tokenizer\n",
    ")\n",
    "\n",
    "model_path = \"models/bin_one_model\"\n",
    "model_bin_one = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# rebuild the Trainer (with same args/tokenizer you used before)\n",
    "trainer_bin_one = Trainer(\n",
    "    model=model_bin_one,\n",
    "    args=training_args_bin_one,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=tokenized_train_bin_one,\n",
    "    eval_dataset=tokenized_dev_bin_one,\n",
    "    processing_class = tokenizer\n",
    ")\n",
    "\n",
    "model_path = \"models/multi_maj_model\"\n",
    "model_multi_maj = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# rebuild the Trainer (with same args/tokenizer you used before)\n",
    "trainer_multi_maj = Trainer(\n",
    "    model=model_multi_maj,\n",
    "    args=training_args_multi_maj,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=tokenized_train_multi_maj,\n",
    "    eval_dataset=tokenized_dev_multi_maj,\n",
    "    processing_class = tokenizer\n",
    ")\n",
    "\n",
    "model_path = \"models/disagree_bin_model\"\n",
    "model_disagree_bin = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# rebuild the Trainer (with same args/tokenizer you used before)\n",
    "trainer_disagree_bin = Trainer(\n",
    "    model=model_disagree_bin,\n",
    "    args=training_args_disagree_bin,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=tokenized_train_disagree_bin,\n",
    "    eval_dataset=tokenized_dev_disagree_bin,\n",
    "    processing_class = tokenizer\n",
    ")"
   ],
   "id": "5bde843ee1221104",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Now we can make some predictions",
   "id": "ebf7177e9e8e18e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:09:21.316773Z",
     "start_time": "2025-08-23T08:09:21.312266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def determine_label(prediction):\n",
    "  labels = []\n",
    "  for score in prediction[0]:\n",
    "    labels.append(np.where(score == max(score))[0][0])\n",
    "  return labels"
   ],
   "id": "1976d563acde0240",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# First let's check the dev sets, since here we have reference data (though there is actually no need to do this, as we already know the outcome - but good to see it works)",
   "id": "33ae067b50b9f8cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:18:27.699848Z",
     "start_time": "2025-08-23T08:09:21.332804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dev_predictions_bin_maj = trainer_bin_maj.predict(test_dataset=tokenized_dev_bin_maj)\n",
    "dev_predictions_bin_one = trainer_bin_one.predict(test_dataset=tokenized_dev_bin_one)\n",
    "dev_predictions_bin_all = trainer_bin_all.predict(test_dataset=tokenized_dev_bin_all)\n",
    "dev_predictions_multi_maj = trainer_multi_maj.predict(test_dataset=tokenized_dev_multi_maj)\n",
    "dev_predictions_disagree_bin = trainer_disagree_bin.predict(test_dataset=tokenized_dev_disagree_bin)"
   ],
   "id": "c3bdf79fbe5a3878",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:18:27.840228Z",
     "start_time": "2025-08-23T08:18:27.826688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Dev set F1 score Bin Maj: {dev_predictions_bin_maj.metrics['test_f1']:.4f}\")\n",
    "print(f\"Dev set F1 score Bin One: {dev_predictions_bin_one.metrics['test_f1']:.4f}\")\n",
    "print(f\"Dev set F1 score Bin All: {dev_predictions_bin_all.metrics['test_f1']:.4f}\")\n",
    "print(f\"Dev set F1 score Multi Maj: {dev_predictions_multi_maj.metrics['test_f1']:.4f}\")\n",
    "print(f\"Dev set F1 score Disagree Bin: {dev_predictions_disagree_bin.metrics['test_f1']:.4f}\")"
   ],
   "id": "a10def0638645e55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev set F1 score Bin Maj: 0.7610\n",
      "Dev set F1 score Bin One: 0.7572\n",
      "Dev set F1 score Bin All: 0.8308\n",
      "Dev set F1 score Multi Maj: 0.6532\n",
      "Dev set F1 score Disagree Bin: 0.6741\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Let's make predictions for the testset now. ",
   "id": "d974ba7411dcc91d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:25:56.732076Z",
     "start_time": "2025-08-23T08:18:27.898595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_test_bin_maj = tokenized_test_bin_maj.remove_columns(['label'])\n",
    "test_predictions_bin_maj = trainer_bin_maj.predict(test_dataset=tokenized_test_bin_maj)\n",
    "tokenized_test_bin_one = tokenized_test_bin_one.remove_columns(['label'])\n",
    "test_predictions_bin_one = trainer_bin_one.predict(test_dataset=tokenized_test_bin_one)\n",
    "tokenized_test_bin_all = tokenized_test_bin_all.remove_columns(['label'])\n",
    "test_predictions_bin_all = trainer_bin_all.predict(test_dataset=tokenized_test_bin_all)\n",
    "tokenized_test_multi_maj = tokenized_test_multi_maj.remove_columns(['label'])\n",
    "test_predictions_multi_maj = trainer_multi_maj.predict(test_dataset=tokenized_test_multi_maj)\n",
    "tokenized_test_disagree_bin = tokenized_test_disagree_bin.remove_columns(['label'])\n",
    "test_predictions_disagree_bin = trainer_disagree_bin.predict(test_dataset=tokenized_test_disagree_bin)"
   ],
   "id": "9654780d8ab296f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:25:56.882685Z",
     "start_time": "2025-08-23T08:25:56.873584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def determine_label(prediction):\n",
    "  labels = []\n",
    "  for score in prediction[0]:\n",
    "    labels.append(np.where(score == max(score))[0][0])\n",
    "  return labels"
   ],
   "id": "5869d5f3f95bb48b",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:25:56.963988Z",
     "start_time": "2025-08-23T08:25:56.945376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predicted_bin_maj_labels_test_set = determine_label(test_predictions_bin_maj)\n",
    "predicted_bin_one_labels_test_set = determine_label(test_predictions_bin_one)\n",
    "predicted_bin_all_labels_test_set = determine_label(test_predictions_bin_all)\n",
    "predicted_multi_maj_labels_test_set = determine_label(test_predictions_multi_maj)\n",
    "predicted_disagree_bin_labels_test_set = determine_label(test_predictions_disagree_bin)"
   ],
   "id": "736112b95f577573",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:25:57.046238Z",
     "start_time": "2025-08-23T08:25:57.039318Z"
    }
   },
   "cell_type": "code",
   "source": "predicted_multi_maj_labels_test_set_with_label =[['0-Kein', '1-Gering', '2-Vorhanden', '3-Stark', '4-Extrem'][i] for i in predicted_multi_maj_labels_test_set]",
   "id": "3febdbe69d5ac322",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T08:25:57.177941Z",
     "start_time": "2025-08-23T08:25:57.120428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "id_test = tokenized_test_bin_maj['id']\n",
    "\n",
    "rows = zip(id_test,\n",
    "           predicted_bin_maj_labels_test_set,\n",
    "           predicted_bin_one_labels_test_set,\n",
    "           predicted_bin_all_labels_test_set,\n",
    "           predicted_multi_maj_labels_test_set_with_label,\n",
    "           predicted_disagree_bin_labels_test_set)\n",
    "\n",
    "header = [\"id\", \"bin_maj\", \"bin_one\", \"bin_all\", \"multi_maj\", \"disagree_bin\"]\n",
    "\n",
    "# Write to CSV file\n",
    "with open('test.csv', 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "\n",
    "    # Write the header\n",
    "    csvwriter.writerow(header)\n",
    "\n",
    "    # Write the data\n",
    "    csvwriter.writerows(rows)"
   ],
   "id": "aa2effe7661319d",
   "outputs": [],
   "execution_count": 49
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

